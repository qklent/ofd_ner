{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab4ed5a3-5bf5-4e89-91e9-1bedc3bd6070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "import string\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a36845c-ab98-41c9-b809-a21f70c77a73",
   "metadata": {},
   "source": [
    "https://huggingface.co/Babelscape/wikineural-multilingual-ner\n",
    "\n",
    "https://huggingface.co/IlyaGusev/rubert_ext_sum_gazeta\n",
    "\n",
    " tgpt \"let's assume that i trained BERT for masked language modeling. Now i wan't to fine-tune for named entity recognition. How should i change my model architecture to do so?\"\n",
    "let's assume that i trained BERT for masked language modeling. Now i wan't to fine-tune for named entity recognition. How should i change my model architecture to do so?\n",
    "\n",
    "To fine-tune a pre-trained BERT model for Named Entity Recognition (NER), you would need to make\n",
    " some modifications to the model architecture. Here are the steps you can follow:\n",
    "\n",
    "1. Load the pre-trained BERT model and add an additional layer on top of it for NER.\n",
    "2. Replace the final layer of the pre-trained BERT model with a new classification layer that\n",
    " predicts the entity label for each token in the input sequence.\n",
    "3. Modify the loss function to calculate the cross-entropy loss between the predicted entity\n",
    " labels and the true entity labels for each token.\n",
    "4. Fine-tune the entire model on your NER dataset by updating the weights of all layers, including\n",
    " the pre-trained BERT layers and the new classification layer.\n",
    "\n",
    "Here's some sample code to help you get started:\n",
    "\n",
    "```python\n",
    "from transformers import BertForTokenClassification, BertTokenizer\n",
    "\n",
    "# Load pre-trained BERT model for masked language modeling\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Add a new classification layer for NER\n",
    "num_labels = 4 # Number of entity labels in your dataset\n",
    "ner_model = BertForTokenClassification.from_pretrained(\n",
    "    'bert-base-uncased', \n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# Replace the final layer of the pre-trained BERT model with the new classification layer\n",
    "ner_model.bert = model.bert\n",
    "ner_model.classifier = nn.Linear(768, num_labels)\n",
    "\n",
    "# Modify the loss function to calculate the cross-entropy loss between predicted and true entity\n",
    " labels\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tune the entire model on your NER dataset\n",
    "optim = AdamW(ner_model.parameters(), lr=5e-5)\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        outputs = ner_model(**inputs)\n",
    "        loss = loss_fn(outputs.logits.view(-1, num_labels), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "```\n",
    "\n",
    "In this example code, num_labels represents the number of entity labels in your dataset. You\n",
    " would need to replace it with the actual number of entity labels in your NER dataset.\n",
    "\n",
    "I hope this helps! Let me know if you have any further questions or concerns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e798bd14-e55c-4923-afcd-e45faa944eee",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "127209ba-95b7-4496-a35e-68a86378b037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Зубная щетка Орал Би Три эффект Деликатное отб...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>салфетки VISTER влажные для ко</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Платье женское DR8517K 7Л8999 Светло-серый 449...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ЛАКОМСТВО \"ДЕРЕВЕНСКИЕ ЛАКОМСТВА\" д/собак мини...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Суппорт гитарный Ergo Play Troster</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               name\n",
       "0   0  Зубная щетка Орал Би Три эффект Деликатное отб...\n",
       "1   1                     салфетки VISTER влажные для ко\n",
       "2   2  Платье женское DR8517K 7Л8999 Светло-серый 449...\n",
       "3   3  ЛАКОМСТВО \"ДЕРЕВЕНСКИЕ ЛАКОМСТВА\" д/собак мини...\n",
       "4   4                 Суппорт гитарный Ergo Play Troster"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/train_unsupervised_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f1fe494-5f2d-4b7c-9b6d-2092631d5f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b1f310b-4720-49b1-ae9b-d42389a79ca3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#функции для препроцесса\n",
    "#тут можно еще сделать ru_to_eng(дада и такие странные примеры есть)\n",
    "def eng_to_ru(word: str) -> str:\n",
    "    if len(word) <= 1:\n",
    "        return word\n",
    "    a = ord('а')\n",
    "    ru_alphabet = ''.join([chr(i) for i in range(a,a+33)])\n",
    "    eng_alphabet = string.ascii_lowercase[:26]\n",
    "    change = {\n",
    "        \"a\": \"а\",\n",
    "        \"e\": \"е\",\n",
    "        \"o\": \"о\",\n",
    "        \"k\": \"к\",\n",
    "        \"3\": \"з\",\n",
    "        \"p\": \"р\",\n",
    "        \"c\": \"с\",\n",
    "        \"m\": \"м\",\n",
    "        \"x\": \"х\",\n",
    "        \"t\": \"т\",\n",
    "        \"y\": \"у\",\n",
    "        \"z\": \"з\",\n",
    "    }\n",
    "\n",
    "    new_word = \"\"\n",
    "    for i in range(len(word)):\n",
    "        curr = word[i]\n",
    "        if curr in eng_alphabet:\n",
    "            if i - 1 < 0:\n",
    "                prev_letter = \"j\"\n",
    "            else:\n",
    "                prev_letter = word[i - 1]\n",
    "\n",
    "            if i + 1 >= len(word):\n",
    "                next_letter = \"j\"\n",
    "            else:\n",
    "                next_letter = word[i + 1]\n",
    "\n",
    "            if next_letter in ru_alphabet or prev_letter in ru_alphabet:\n",
    "                curr = change.get(curr, curr)\n",
    "        new_word += curr\n",
    "\n",
    "    return new_word\n",
    "\n",
    "def only_ones(word: str) -> str:\n",
    "    if word.isdigit() or re.match(r'^-?\\d+(?:\\.\\d+)$', word):\n",
    "        return \"1\"\n",
    "    else:\n",
    "        new_word = \"\"\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            curr = word[i]\n",
    "            while i < len(word) and word[i].isdigit():\n",
    "                curr = \"1\"\n",
    "                i += 1\n",
    "            new_word += curr\n",
    "            if curr != \"1\":\n",
    "                i += 1\n",
    "\n",
    "    return new_word\n",
    "\n",
    "def remove_punct(word: str) -> str:\n",
    "    word = re.sub(r'[^\\w\\s]+', ' ', word)\n",
    "    word = re.sub(r'\\s+', ' ', word)\n",
    "    return word.strip()\n",
    "\n",
    "\n",
    "def preprocess_string(word: str) -> str:\n",
    "    new_word = \"\"\n",
    "\n",
    "\n",
    "    word = remove_punct(word)\n",
    "\n",
    "    if word != \"товара нет\":\n",
    "        new_word = eng_to_ru(only_ones(word).lower())\n",
    "\n",
    "    return new_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47857469-41d1-4c84-8c0a-d71fddd04670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>зубная щетка орал би три эффект деликатное отб...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>салфетки vister влажные для ко</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>платье женское dr1k 1л1 светло серый 1 1 1 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>лакомство деревенские лакомства д собак мини п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>суппорт гитарный ergo play troster</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               name\n",
       "0   0  зубная щетка орал би три эффект деликатное отб...\n",
       "1   1                     салфетки vister влажные для ко\n",
       "2   2       платье женское dr1k 1л1 светло серый 1 1 1 1\n",
       "3   3  лакомство деревенские лакомства д собак мини п...\n",
       "4   4                 суппорт гитарный ergo play troster"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.fillna(\"\")\n",
    "df.name = df.name.apply(preprocess_string)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dec0f882-ed55-4388-b7f8-ffd21efa4174",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df)\n",
    "train = df[:int(n * 0.8)]\n",
    "val = df[int(n * 0.8):]\n",
    "dataset_dict = {\"train\": train, \"validation\": val}\n",
    "\n",
    "train_dataset = Dataset.from_dict(train)\n",
    "validation_dataset = Dataset.from_dict(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9de6ba45-0bf4-4a5d-b25a-7d923f8a7ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'name'],\n",
       "    num_rows: 800000\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4a45854-37f6-42db-9c31-9bc46088f627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'name'],\n",
       "        num_rows: 800000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'name'],\n",
       "        num_rows: 200000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset\n",
    "})\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d721fc-fa8b-445d-a6fb-06f1f08cbe24",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "497b9ee9-4433-49fc-95c7-cc92eae58809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|███████████████████████| 632/632 [00:00<00:00, 1.51MB/s]\n",
      "Downloading model.safetensors: 100%|█████████████████████████| 47.7M/47.7M [00:04<00:00, 10.1MB/s]\n",
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading (…)okenizer_config.json: 100%|████████████████████████| 341/341 [00:00<00:00, 779kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 241kB [00:00, 1.13MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 468kB [00:00, 1.94MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|████████████████████████| 112/112 [00:00<00:00, 393kB/s]\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"cointegrated/rubert-tiny\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "# model.cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2111dc20-61c3-499e-a573-44e147d60628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 800000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 200000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"name\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = dataset_dict.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"name\", \"id\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b8cae34-8d88-4c71-ae14-8a6d7b670f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3edc54e5-1d43-4c2d-9e39-bc7522f9ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c201fe72-fbbb-4f80-94bd-92cda2c29441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Review 0 length: 26'\n",
      "'>>> Review 1 length: 13'\n",
      "'>>> Review 2 length: 21'\n"
     ]
    }
   ],
   "source": [
    "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Review {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbdaec7b-1d93-4b0e-a9d7-1963dc757f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Concatenated reviews length: 60'\n"
     ]
    }
   ],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5fd4896-fc7c-4f33-b7b6-9322a7bbb844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 60'\n"
     ]
    }
   ],
   "source": [
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da5da013-1063-431c-9195-2f0aa3397eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5ba2221b-b91b-42f1-96ff-39e8b2d44772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 107221\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 26795\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb673ad4-1768-4b3e-abaf-ab0070479125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##окколи [SEP] [CLS] малавиткрем гель д лица пит увлаж 1мл [SEP] [CLS] 1 1 майка женс dfy dws1 [SEP] [CLS] 1 sko корсет майка джинсово синий l [SEP] [CLS] 1 сумка подарочная 1 1мм bc [SEP] [CLS] салфетки фортуна 1 1 целлюлозные 1шт [SEP] [CLS] 1 1 1 ринонорм спрей наз для взрослых 1 1 1мл [SEP] [CLS] а диски replay mercedes mr1 1 1 r1 pcd 1 1 et 1'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31fe35f5-c828-468b-be05-4118507a092b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##окколи [SEP] [CLS] малавиткрем гель д лица пит увлаж 1мл [SEP] [CLS] 1 1 майка женс dfy dws1 [SEP] [CLS] 1 sko корсет майка джинсово синий l [SEP] [CLS] 1 сумка подарочная 1 1мм bc [SEP] [CLS] салфетки фортуна 1 1 целлюлозные 1шт [SEP] [CLS] 1 1 1 ринонорм спрей наз для взрослых 1 1 1мл [SEP] [CLS] а диски replay mercedes mr1 1 1 r1 pcd 1 1 et 1'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "927f922c-c0ab-4198-8cc8-75ec29f760c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "08ce309e-f074-48da-888b-817b7ef2af26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] зубная щетка орал би три эффект деликатное отбе [MASK]ние 1 средняя 1штап 1 [SEP] [CLS] салфетки vis [MASK] влажные для ко [SEP] [CLS] платье [MASK]нское dr1k 1л1 светло серый 1 1водят 1 [SEP] [CLS] лакомство деревенские лакомrrow [MASK] [MASK]бак мини пород косточки из полковникдейки 1 [MASK] [SEP] [CLS] су [MASK]портболтарный [MASK] [MASK] play [MASK]ster [SEP] [CLS] котлет [MASK] из индейки мираторг 1г [SEP] [CLS] 1 салат из крабов [MASK] пало [MASK] и бр'\n",
      "\n",
      "'>>> ##окколи [SEP] [CLS] малавитк [MASK] гель д лица пит увлаж [MASK] [MASK]л [SEP] [CLS] 1 1 майка женс dfy dws1 [SEP] [CLS] 1 sko кор [MASK] [MASK] джинсово синий [MASK] [SEP] [CLS] 1 сумка подарочная 1 1мм bc [SEP] [CLS] салфетки фортуна 1 1 целлюлозные 1шт [SEP] [CLS] 1 1 1 ринонорм спрей наз для взрос [MASK] 1 1 1мл [SEP] [CLS] а диски replay mercedes mr1 1 1 r1 pc [MASK] 1 1 et 1'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aa55d95c-ce79-4620-ad5c-c1809869fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "93762f82-04c5-4434-81c8-887f40764a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] зубная щетка [MASK] [MASK] би три [MASK] [MASK] деликатное отбеливание 1 [MASK] 1шт 1 1 [SEP] [CLS] салфетки [MASK] [MASK] влажные для ко [SEP] [CLS] платье женское dr1k [MASK] [MASK] [MASK] [MASK] [MASK] серый 1 [MASK] 1 1 [SEP] [CLS] лакомство [MASK] [MASK] [MASK] лакомства д собак мини пород косточки [MASK] индейки 1г [SEP] [CLS] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] ergo play troster [SEP] [CLS] котлеты из индейки мираторг 1г [SEP] [CLS] 1 [MASK] [MASK] из крабовых палочек и бр'\n",
      "\n",
      "'>>> ##окколи [SEP] [CLS] малавиткрем гель д лица [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] 1мл [SEP] [CLS] 1 1 майка [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [SEP] [CLS] 1 sko корсет [MASK] джинсово синий l [SEP] [CLS] 1 сумка подарочная 1 1мм bc [SEP] [CLS] салфетки [MASK] [MASK] [MASK] 1 1 целлюлозные 1шт [SEP] [CLS] [MASK] 1 1 ринонорм спрей [MASK] [MASK] для взрослых [MASK] 1 1мл [SEP] [CLS] а диски replay mercedes [MASK] [MASK] [MASK] [MASK] 1 r1 pcd 1 1 [MASK] 1'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034abdc4-4e45-4431-827e-58ace2c9e26a",
   "metadata": {},
   "source": [
    "# downsample dataest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4c814611-513e-4a5e-a584-1ed8cd453584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 50_000\n",
    "test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=1337\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09b8f53-0a74-4009-916a-ec45848c2b59",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e513f375-09f1-4e39-98a3-571d4bd36b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">8</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>logging_steps = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(downsampled_dataset[<span style=\"color: #808000; text-decoration-color: #808000\">\"train\"</span>]) // batch_size                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>model_name = model_checkpoint.split(<span style=\"color: #808000; text-decoration-color: #808000\">\"/\"</span>)[-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>]                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 8 training_args = TrainingArguments(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>output_dir=<span style=\"color: #808000; text-decoration-color: #808000\">f\"{</span>model_name<span style=\"color: #808000; text-decoration-color: #808000\">}-finetuned-imdb\"</span>,                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>overwrite_output_dir=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>,                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>evaluation_strategy=<span style=\"color: #808000; text-decoration-color: #808000\">\"epoch\"</span>,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">111</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/qklent/programming/machine_learning/alfa_bank_receipts/.venv/lib/python3.11/site-packages/</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">training_args.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1344</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__post_init__</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1341 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> (get_xla_device_type(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.device) != <span style=\"color: #808000; text-decoration-color: #808000\">\"GPU\"</span>)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1342 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.fp16 <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.fp16_full_eval)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1343 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>):                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1344 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1345 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1346 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\" (`--fp16_full_eval`) can only be used on CUDA devices.\"</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1347 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>FP16 Mixed precision training with AMP or APEX <span style=\"font-weight: bold\">(</span>`--fp16`<span style=\"font-weight: bold\">)</span> and FP16 half precision evaluation \n",
       "<span style=\"font-weight: bold\">(</span>`--fp16_full_eval`<span style=\"font-weight: bold\">)</span> can only be used on CUDA devices.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m8\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0mlogging_steps = \u001b[96mlen\u001b[0m(downsampled_dataset[\u001b[33m\"\u001b[0m\u001b[33mtrain\u001b[0m\u001b[33m\"\u001b[0m]) // batch_size                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0mmodel_name = model_checkpoint.split(\u001b[33m\"\u001b[0m\u001b[33m/\u001b[0m\u001b[33m\"\u001b[0m)[-\u001b[94m1\u001b[0m]                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 8 training_args = TrainingArguments(                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2m│   \u001b[0moutput_dir=\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mmodel_name\u001b[33m}\u001b[0m\u001b[33m-finetuned-imdb\u001b[0m\u001b[33m\"\u001b[0m,                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m\u001b[2m│   \u001b[0moverwrite_output_dir=\u001b[94mTrue\u001b[0m,                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[2m│   \u001b[0mevaluation_strategy=\u001b[33m\"\u001b[0m\u001b[33mepoch\u001b[0m\u001b[33m\"\u001b[0m,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m__init__\u001b[0m:\u001b[94m111\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/qklent/programming/machine_learning/alfa_bank_receipts/.venv/lib/python3.11/site-packages/\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mtransformers/\u001b[0m\u001b[1;33mtraining_args.py\u001b[0m:\u001b[94m1344\u001b[0m in \u001b[92m__post_init__\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1341 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[95mand\u001b[0m (get_xla_device_type(\u001b[96mself\u001b[0m.device) != \u001b[33m\"\u001b[0m\u001b[33mGPU\u001b[0m\u001b[33m\"\u001b[0m)                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1342 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[95mand\u001b[0m (\u001b[96mself\u001b[0m.fp16 \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m.fp16_full_eval)                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1343 \u001b[0m\u001b[2m│   │   \u001b[0m):                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1344 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1345 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1346 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m (`--fp16_full_eval`) can only be used on CUDA devices.\u001b[0m\u001b[33m\"\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1347 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0mFP16 Mixed precision training with AMP or APEX \u001b[1m(\u001b[0m`--fp16`\u001b[1m)\u001b[0m and FP16 half precision evaluation \n",
       "\u001b[1m(\u001b[0m`--fp16_full_eval`\u001b[1m)\u001b[0m can only be used on CUDA devices.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-imdb\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21db24b2-c512-4998-8863-38657a2d13d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a903f535-0013-4014-a647-4c33c9d7d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e1747c-c439-4ee0-bd6a-c8098df8f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6258236f-82c1-4afc-bd5c-e671f1f2db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
